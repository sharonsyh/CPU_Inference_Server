# CPU_Inference_Server

This project implements a Flask-based API to run inference on Meta's LLaMA 3 model entirely on a CPU. The server takes conversation dialogues as input and generates responses using a pre-trained LLaMA 3 model.

ðŸ›  Features
Flask API for text generation
LLaMA 3 model inference on CPU
Chat completion support for multi-turn conversations
Configurable parameters (e.g., max sequence length, temperature, top-p sampling)
JSON-based API requests and responses
